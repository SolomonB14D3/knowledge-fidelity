{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Fidelity Demo\n",
    "\n",
    "**Compress an LLM and audit whether it still knows truth vs popular myths.**\n",
    "\n",
    "This notebook demonstrates the full pipeline:\n",
    "1. Audit a model's confidence on true vs false statements (baseline)\n",
    "2. Compress with CF90 (SVD + layer freezing)\n",
    "3. Re-audit and compare: how much signal was preserved?\n",
    "\n",
    "Runtime: ~2 min on Qwen-0.5B (CPU), ~15 min on 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from knowledge_fidelity import compress_and_audit, get_default_probes, get_mandela_probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. One-Call Compress + Audit\n",
    "\n",
    "The simplest way to use the toolkit. One function call that:\n",
    "- Loads the model\n",
    "- Measures confidence BEFORE compression\n",
    "- Applies CF90 (SVD compress Q/K/O at 70% rank, freeze 75% of layers)\n",
    "- Measures confidence AFTER compression\n",
    "- Returns a full report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = compress_and_audit(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    ratio=0.7,\n",
    "    freeze_ratio=0.75,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Retention:   {report['retention']:.0%}\")\n",
    "print(f\"rho before:  {report['rho_before']:.3f}\")\n",
    "print(f\"rho after:   {report['rho_after']:.3f}\")\n",
    "print(f\"rho drop:    {report['rho_before'] - report['rho_after']:.3f}\")\n",
    "print(f\"Compressed:  {report['compression']['n_compressed']} matrices\")\n",
    "print(f\"Frozen:      {report['freeze']['n_frozen']}/{report['freeze']['n_layers']} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Per-Probe Breakdown\n",
    "\n",
    "Let's look at which probes retained their signal and which didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "probes = get_default_probes()\n",
    "audit = report['audit_after']\n",
    "\n",
    "print(f\"{'Probe ID':<25} {'Delta':>8} {'True Conf':>10} {'False Conf':>11} {'Status':>8}\")\n",
    "print(\"-\" * 65)\n",
    "for i, p in enumerate(probes):\n",
    "    delta = audit['deltas'][i]\n",
    "    tc = audit['true_confs'][i]\n",
    "    fc = audit['false_confs'][i]\n",
    "    status = 'OK' if delta > 0 else 'FLIP'\n",
    "    print(f\"{p['id']:<25} {delta:>+8.4f} {tc:>10.3f} {fc:>11.3f} {status:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Before vs After\n",
    "\n",
    "Compare confidence distributions before and after compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 5)\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before compression\n",
    "ax = axes[0]\n",
    "before = report['audit_before']\n",
    "x = np.arange(len(probes))\n",
    "ax.bar(x - 0.2, before['true_confs'], 0.4, label='True', color='#2ecc71', alpha=0.8)\n",
    "ax.bar(x + 0.2, before['false_confs'], 0.4, label='False', color='#e74c3c', alpha=0.8)\n",
    "ax.set_title(f'BEFORE Compression (rho={report[\"rho_before\"]:.3f})', fontsize=13)\n",
    "ax.set_ylabel('Mean Confidence')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([p['id'][:12] for p in probes], rotation=45, ha='right', fontsize=7)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, max(max(before['true_confs']), max(before['false_confs'])) * 1.15)\n",
    "\n",
    "# After compression\n",
    "ax = axes[1]\n",
    "after = report['audit_after']\n",
    "ax.bar(x - 0.2, after['true_confs'], 0.4, label='True', color='#2ecc71', alpha=0.8)\n",
    "ax.bar(x + 0.2, after['false_confs'], 0.4, label='False', color='#e74c3c', alpha=0.8)\n",
    "ax.set_title(f'AFTER Compression (rho={report[\"rho_after\"]:.3f})', fontsize=13)\n",
    "ax.set_ylabel('Mean Confidence')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([p['id'][:12] for p in probes], rotation=45, ha='right', fontsize=7)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, max(max(after['true_confs']), max(after['false_confs'])) * 1.15)\n",
    "\n",
    "plt.suptitle('Knowledge Fidelity: Confidence Before vs After CF90 Compression', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/demo_before_after.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Delta Distribution\n",
    "\n",
    "The confidence delta (true - false) should stay positive after compression.\n",
    "A positive delta means the model is still more confident about truth than myth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "deltas_before = np.array(report['audit_before']['deltas'])\n",
    "deltas_after = np.array(report['audit_after']['deltas'])\n",
    "\n",
    "x = np.arange(len(probes))\n",
    "ax.bar(x - 0.2, deltas_before, 0.4, label='Before', color='#3498db', alpha=0.8)\n",
    "ax.bar(x + 0.2, deltas_after, 0.4, label='After CF90', color='#e67e22', alpha=0.8)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_ylabel('Confidence Delta (true - false)')\n",
    "ax.set_title('Confidence Delta Preservation Under CF90 Compression')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([p['id'][:12] for p in probes], rotation=45, ha='right', fontsize=7)\n",
    "ax.legend()\n",
    "\n",
    "n_preserved = sum(1 for d in deltas_after if d > 0)\n",
    "ax.annotate(f'{n_preserved}/{len(probes)} probes preserved',\n",
    "           xy=(0.98, 0.95), xycoords='axes fraction', ha='right', va='top',\n",
    "           fontsize=11, fontweight='bold',\n",
    "           bbox=dict(boxstyle='round,pad=0.3', facecolor='#2ecc71', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/demo_delta_preservation.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mandela Effect Probes\n",
    "\n",
    "Test on popular false memories. These are claims that many people (and LLMs) get wrong.\n",
    "\n",
    "Note: The Mandela effect signal strengthens with model scale. At 0.5B you may not see\n",
    "strong separation; at 6.9B+ the effect is highly significant (p=0.016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_fidelity import audit_model, get_mandela_probes\n",
    "\n",
    "mandela = get_mandela_probes()\n",
    "mandela_audit = audit_model(report['model'], report['tokenizer'], probes=mandela)\n",
    "\n",
    "print(f\"Mandela probes (post-compression):\")\n",
    "print(f\"  rho={mandela_audit['rho']:.3f} (p={mandela_audit['rho_p']:.4f})\")\n",
    "print(f\"  {mandela_audit['n_positive_delta']}/{mandela_audit['n_probes']} correct\")\n",
    "print()\n",
    "for i, p in enumerate(mandela):\n",
    "    d = mandela_audit['deltas'][i]\n",
    "    marker = 'correct' if d > 0 else 'WRONG'\n",
    "    print(f\"  {p['id']:<30s} delta={d:+.4f}  [{marker}]\")\n",
    "    if 'note' in p:\n",
    "        print(f\"    Note: {p['note']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using the Compressed Model\n",
    "\n",
    "The compressed model is still a standard HuggingFace model â€” use it normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = report['model']\n",
    "tokenizer = report['tokenizer']\n",
    "\n",
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"Water boils at\",\n",
    "    \"Einstein developed the theory of\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to('cpu')\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=10, do_sample=False,\n",
    "                            pad_token_id=tokenizer.pad_token_id)\n",
    "    response = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    print(f\"{prompt} -> {response[len(prompt):].strip()[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Larger models**: Try `meta-llama/Llama-3.1-8B-Instruct` for stronger Mandela effect signal\n",
    "- **Custom probes**: `compress_and_audit(model, probes=your_probes)` with domain-specific facts\n",
    "- **Aggressive compression**: Set `use_importance=True` for importance-guided SVD at ratios below 70%\n",
    "- **Export**: Save with `output_dir='./compressed'` then use `deployment/export_gguf.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
